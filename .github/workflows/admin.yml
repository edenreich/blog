name: admin

on:
  push:
    branches:
      - master
      - develop
    paths:
      - ".github/workflows/admin.yml"
      - "admin/**"

env:
  DOCKER_BUILDKIT: 1
  DOCKER_REPO: edenr/blog-admin 
  DOCKER_REPO_DEV: edenr/blog-admin-devtools
  STAGING_SERVERS: k8s-master k8s-node1 k8s-node2 k8s-node3 k8s-node4 k8s-node5 k8s-node6
  PRODUCTION_SERVERS: k4s-master k4s-node1 k4s-node2 k4s-node3 k4s-node4
  DEPLOYMENT_NAME: admin
  DEPLOYMENT_VERSION: $(echo ${{ github.event.after }} | cut -c1-8)
  DEPLOYMENT_PREVIOUS_VERSION: $(echo ${{ github.event.before }} | cut -c1-8)

jobs:
  docker_login:
    runs-on: self-hosted
    steps:
      - name: Add docker login credentials to github-runner server
        run: |
          echo ${{ secrets.DOCKER_CONFIG_JSON }} | base64 -d > ~/.docker/config.json
          docker login
  update_configuration_staging:
    runs-on: self-hosted
    environment:
      name: staging
    steps:
      - name: Checkout
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
      - name: Kubernetes - switch to staging context
        run: |
          kubectl config use-context staging
      - name: Kubernetes - update configmap in the cluster
        run: |
          APP_ENV=staging \
          envsubst < admin/config.yaml | kubectl apply -f -
  update_configuration_production:
    if: github.event.ref == 'refs/heads/master'
    runs-on: self-hosted
    environment:
      name: production
    steps:
      - uses: actions/checkout@master
      - name: Kubernetes - switch to production context
        run: |
          kubectl config use-context production
      - name: Kubernetes - update configmap in the cluster
        run: |
          APP_ENV=prod \
          envsubst < admin/config.yaml | kubectl apply -f -
  update_secrets_staging:
    runs-on: self-hosted
    environment:
      name: staging
    steps:
      - name: Checkout
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
      - name: Kubernetes - switch to staging context
        run: |
          kubectl config use-context staging
      - name: Kubernetes - update secrets in the cluster
        run: |
          APP_SECRET=${{ secrets.ADMIN_APP_SECRET }} \
          DATABASE_URL=${{ secrets.ADMIN_DATABASE_URL }} \
          envsubst < admin/secret.yaml | kubectl apply -f -
      - name: Kubernetes - add docker pull secrets if not exists
        run: |
          DOCKER_CONFIG_JSON=${{ secrets.DOCKER_CONFIG_JSON }} \
          envsubst < ops/on-premises/kubernetes/secrets/container_registry.yaml | kubectl apply -f -
  update_secrets_production:
    if: github.event.ref == 'refs/heads/master'
    runs-on: self-hosted
    environment:
      name: production
    steps:
      - name: Checkout
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
      - name: Kubernetes - switch to production context
        run: |
          kubectl config use-context production
      - name: Kubernetes - update secrets in the cluster
        run: |
          APP_SECRET=${{ secrets.ADMIN_APP_SECRET }} \
          DATABASE_URL=${{ secrets.ADMIN_DATABASE_URL }} \
          envsubst < admin/secret.yaml | kubectl apply -f -
      - name: Kubernetes - add docker pull secrets if not exists
        run: |
          DOCKER_CONFIG_JSON=${{ secrets.DOCKER_CONFIG_JSON }} \
          envsubst < ops/on-premises/kubernetes/secrets/container_registry.yaml | kubectl apply -f -
  build:
    runs-on: self-hosted
    needs: docker_login
    steps:
      - name: Checkout
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
      - name: Docker - build container image
        run: |
          cd admin && docker build \
            --target production \
            --cache-from ${{ env.DOCKER_REPO }}:latest \
            -t ${{ env.DOCKER_REPO }}:latest \
            -t ${{ env.DOCKER_REPO }}:${{ env.DEPLOYMENT_VERSION }} \
            .
  push:
    needs: build
    runs-on: self-hosted
    steps:
      - name: Docker - push image to registry
        run: |
          docker push ${{ env.DOCKER_REPO }}:${{ env.DEPLOYMENT_VERSION }}
          docker push ${{ env.DOCKER_REPO }}:latest
  lint:
    needs: push
    runs-on: self-hosted
    environment:
      name: staging
    steps:
      - name: Kubernetes - switch to staging context
        run: |
          kubectl config use-context staging
      - name: Kubernetes - run linter
        run: |
          docker run --rm -it --user node:node -v ${PWD}:/app -w /app node:16.2.0-alpine3.12 yarn lint
  deploy_staging:
    needs: [lint]
    runs-on: self-hosted
    environment:
      name: staging
      url: https://stage-admin.eden-reich.com
    steps:
      - name: Checkout
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
      - name: Kubernetes - switch to staging context
        run: |
          kubectl config use-context staging
      - name: Kubernetes - create a new deployment
        run: |
          VERSION=${{ env.DEPLOYMENT_VERSION }} \
          REPOSITORY=${{ env.DOCKER_REPO }} \
          envsubst < admin/deployment.yaml | kubectl apply -f -
      - name: Kubernetes - wait for new deployment to be successfully rolled out
        run: |
          max_count=60
          count=0
          while [ true ]; do
            if [[ $count -gt $max_count ]]; then
              echo "===> Waited for too long for a successful rolled out green version, but something went wrong. Aborting..."
              exit 1
            fi
            if [[ $(kubectl rollout status deploy/${{ env.DEPLOYMENT_NAME }}-${{ env.DEPLOYMENT_VERSION }} --timeout=5s 2>/dev/null | grep 'successfully rolled out') ]]; then
              echo "=====> Green deployment has been successfully rolled out. Continuing..."
              break
            fi
            count=$((count+1))
            echo "===> Green deployment ${{ env.DEPLOYMENT_NAME }}-${{ env.DEPLOYMENT_VERSION }} is being rolled out...Waiting..."
            sleep 1
          done
      - name: Kubernetes - apply horizontal pods autoscaling to new deployment
        run: |
          VERSION=${{ env.DEPLOYMENT_VERSION }} \
          envsubst < admin/hpa.yaml | kubectl apply -f -
      - name: Kubernetes - switch LB to green deployment
        run: |
          VERSION=${{ env.DEPLOYMENT_VERSION }} \
          envsubst < admin/service.yaml | kubectl apply -f -
  deploy_production:
    if: github.event.ref == 'refs/heads/master'
    needs: deploy_staging
    runs-on: self-hosted
    environment:
      name: production
      url: https://admin.eden-reich.com
    steps:
      - name: Checkout
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
      - name: Kubernetes - switch to production context
        run: |
          kubectl config use-context production
      - name: Kubernetes - create a new deployment
        run: |
          VERSION=${{ env.DEPLOYMENT_VERSION }} \
          REPOSITORY=${{ env.DOCKER_REPO }} \
          envsubst < admin/deployment.yaml | kubectl apply -f -
      - name: Kubernetes - wait for new deployment to be successfully rolled out
        run: |
          max_count=60
          count=0
          while [ true ]; do
            if [[ $count -gt $max_count ]]; then
              echo "===> Waited for too long for a successful rolled out green version, but something went wrong. Aborting..."
              exit 1
            fi
            if [[ $(kubectl rollout status deploy/${{ env.DEPLOYMENT_NAME }}-${{ env.DEPLOYMENT_VERSION }} --timeout=5s 2>/dev/null | grep 'successfully rolled out') ]]; then
              echo "=====> Green deployment has been successfully rolled out. Continuing..."
              break
            fi
            count=$((count+1))
            echo "===> Green deployment ${{ env.DEPLOYMENT_NAME }}-${{ env.DEPLOYMENT_VERSION }} is being rolled out...Waiting..."
            sleep 1
          done
      - name: Kubernetes - apply horizontal pods autoscaling to new deployment
        run: |
          VERSION=${{ env.DEPLOYMENT_VERSION }} \
          envsubst < admin/hpa.yaml | kubectl apply -f -
      - name: Kubernetes - switch LB to green deployment
        run: |
          VERSION=${{ env.DEPLOYMENT_VERSION }} \
          envsubst < admin/service.yaml | kubectl apply -f -
  revert_production:
    needs: deploy_production
    runs-on: self-hosted
    environment:
      name: production
      url: https://admin.eden-reich.com
    steps:
      - name: Kubernetes - switch to production context
        run: |
          kubectl config use-context production
      - name: Kubernetes - apply horizontal pods autoscaling to previous deployment
        run: |
          VERSION=${{ env.DEPLOYMENT_PREVIOUS_VERSION }} \
          envsubst < admin/hpa.yaml | kubectl apply -f -
      - name: Kubernetes - switch LB to previous deployment
        run: |
          VERSION=${{ env.DEPLOYMENT_PREVIOUS_VERSION }} \
          envsubst < admin/service.yaml | kubectl apply -f -
  cleanup:
    runs-on: self-hosted
    needs: deploy_staging
    steps:
      - name: Github-runner - remove all related images except latest
        run: |
          docker rmi -f $(docker images | grep ${{ env.DOCKER_REPO }} | grep -v latest | awk '{image = sprintf("%s:%s", $1, $2); print image}' | xargs) || true
          docker rmi -f $(docker images | grep ${{ env.DOCKER_REPO_DEV }} | grep -v latest | awk '{image = sprintf("%s:%s", $1, $2); print image}' | xargs) || true
      - name: Github-runner - remove dangling docker images
        run: docker system prune -f
      - name: Keep 2 deployments available for rollback
        run: |
          kubectl delete deployments \
            $(kubectl get deployments --sort-by=.metadata.creationTimestamp | grep ${{ env.DEPLOYMENT_NAME }} | awk '{ print $1 }' | head -n -2 | xargs) || true
      - name: Staging worker nodes - remove all dangling images
        if: github.event.ref == 'refs/heads/develop'
        run: |
          for server in $(echo ${{ env.STAGING_SERVERS }} | xargs); do ssh $server ./scripts/cleanup.sh; done
      - name: Production worker nodes - remove all dangling images
        if: github.event.ref == 'refs/heads/master'
        run: |
          for server in $(echo ${{ env.PRODUCTION_SERVERS }} | xargs); do ssh $server ./scripts/cleanup.sh; done
