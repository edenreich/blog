name: frontend

on:
  push:
    branches:
      - master
      - develop 
    paths:
      - ".github/workflows/frontend.yml"
      - "src/frontend/**"
      - "ops/on-premises/docker/frontend/**"
      - "ops/on-premises/kubernetes/frontend/**"

env:
  DOCKER_BUILDKIT: 1
  DOCKER_REPO: edenr/blog-frontend
  STAGING_SERVERS: k8s-master k8s-node1 k8s-node2 k8s-node3 k8s-node4 k8s-node5 k8s-node6
  PRODUCTION_SERVERS: k4s-master k4s-node1 k4s-node2 k4s-node3 k4s-node4
  DEPLOYMENT_NAME: frontend
  DEPLOYMENT_VERSION: $(echo ${{ github.event.after }} | cut -c1-8)
  DEPLOYMENT_PREVIOUS_VERSION: $(echo ${{ github.event.before }} | cut -c1-8)
  ARTIFACT_FILES: src/frontend/package.json src/frontend/yarn.lock ops/on-premises/docker/frontend/artifacts/Dockerfile

jobs:
  docker_login:
    runs-on: self-hosted
    steps:
      - name: Add docker login credentials to github-runner server
        run: |
          echo ${{ secrets.DOCKER_CONFIG_JSON }} | base64 -d > ~/.docker/config.json
          docker login
  update_secrets_staging:
    runs-on: self-hosted
    environment:
      name: staging
    steps:
      - uses: actions/checkout@master
      - name: Kubernetes - switch to staging context
        run: |
          kubectl config use-context staging
      - name: Kubernetes - update secrets in the cluster
        run: |
          FRONTEND_MAILGUN_API_KEY=${{ secrets.FRONTEND_MAILGUN_API_KEY }} \
          FRONTEND_MAILGUN_DOMAIN=${{ secrets.FRONTEND_MAILGUN_DOMAIN }} \
          envsubst < ops/on-premises/kubernetes/secrets/frontend.yaml | kubectl apply -f -
      - name: Kubernetes - add docker pull secrets if not exists
        run: |
          DOCKER_CONFIG_JSON=${{ secrets.DOCKER_CONFIG_JSON }} \
          envsubst < ops/on-premises/kubernetes/secrets/container_registry.yaml | kubectl apply -f -
  check:
    outputs:
      run_job: ${{ steps.check_files.outputs.run_job }}
    runs-on: self-hosted
    steps:
      - uses: actions/checkout@master
        with:
          fetch-depth: 2
      - name: Check if relevant artifact files have been changed
        id: check_files
        run: |
          for file in $(echo ${{ env.ARTIFACT_FILES }} | xargs); do
            if [ $(git diff --name-only HEAD^ HEAD | grep $file) ]; then 
              echo "::set-output name=run_job::true"
              break
            else
              echo "::set-output name=run_job::false"
            fi
          done
  build-artifacts:
    needs: check
    runs-on: self-hosted
    steps:
      - uses: actions/checkout@master
      - name: Docker - build artifacts
        if: ${{ needs.check.outputs.run_job == 'true' }}
        run: |
          docker build \
            -t frontend-artifacts:latest \
            -f ops/on-premises/docker/frontend/artifacts/Dockerfile .
  build:
    needs: [build-artifacts, docker_login]
    runs-on: self-hosted
    steps:
      - uses: actions/checkout@master
      - name: Docker - build container image
        run: |
          docker build \
            --target production \
            --cache-from ${{ env.DOCKER_REPO }}:latest \
            -t ${{ env.DOCKER_REPO }}:latest \
            -t ${{ env.DOCKER_REPO }}:${{ env.DEPLOYMENT_VERSION }} \
            -f ops/on-premises/docker/frontend/Dockerfile .
  push:
    needs: build
    runs-on: self-hosted
    steps:
      - name: Docker - push container image to registry
        run: |
          docker push ${{ env.DOCKER_REPO }}:${{ env.DEPLOYMENT_VERSION }}
          docker push ${{ env.DOCKER_REPO }}:latest
  deploy_staging:
    needs: push
    runs-on: self-hosted
    environment:
      name: staging
      url: https://stage.eden-reich.com
    steps:
      - uses: actions/checkout@master
      - name: Kubernetes - switch to staging context
        run: |
          kubectl config use-context staging
      - name: Kubernetes - create a new deployment
        run: |
          VERSION=${{ env.DEPLOYMENT_VERSION }} \
          REPOSITORY=${{ env.DOCKER_REPO }} \
          envsubst < ops/on-premises/kubernetes/frontend/deployment.yaml | kubectl apply -f -
      - name: Kubernetes - wait for new deployment to be successfully rolled out
        run: |
          max_count=60
          count=0
          while [ true ]; do
            if [[ $count -gt $max_count ]]; then
              echo "===> Waited for too long for a successful rolled out green version, but something went wrong. Aborting..."
              exit 1
            fi
            if [[ $(kubectl rollout status deploy/${{ env.DEPLOYMENT_NAME }}-${{ env.DEPLOYMENT_VERSION }} --timeout=5s 2>/dev/null | grep 'successfully rolled out') ]]; then
              echo "=====> Green deployment has been successfully rolled out. Continuing..."
              break
            fi
            count=$((count+1))
            echo "===> Green deployment ${{ env.DEPLOYMENT_NAME }}-${{ env.DEPLOYMENT_VERSION }} is being rolled out...Waiting..."
            sleep 1
          done
      - name: Kubernetes - apply horizontal pods autoscaling to new deployment
        run: |
          VERSION=${{ env.DEPLOYMENT_VERSION }} \
          envsubst < ops/on-premises/kubernetes/frontend/hpa.yaml | kubectl apply -f -
      - name: Kubernetes - switch LB to green deployment
        run: |
          VERSION=${{ env.DEPLOYMENT_VERSION }} \
          envsubst < ops/on-premises/kubernetes/frontend/service.yaml | kubectl apply -f -
  cleanup:
    runs-on: self-hosted
    needs: deploy_staging
    steps:
      - name: Github-runner - remove all related images except latest
        run: |
          docker rmi -f $(docker images | grep ${{ env.DOCKER_REPO }} | grep -v latest | awk '{image = sprintf("%s:%s", $1, $2); print image}' | xargs) || true
      - name: Github-runner - remove dangling docker images
        run: |
          docker system prune -f
      - name: Staging worker nodes - remove all dangling images
        if: github.event.ref == 'refs/heads/develop'
        run: |
          for server in $(echo ${{ env.STAGING_SERVERS }} | xargs); do ssh $server ./scripts/cleanup.sh; done
      - name: Production worker nodes - remove all dangling images
        if: github.event.ref == 'refs/heads/master'
        run: |
          for server in $(echo ${{ env.PRODUCTION_SERVERS }} | xargs); do ssh $server ./scripts/cleanup.sh; done
      - name: Kubernetes - keep 2 deployments available for rollback
        run: |
          kubectl delete deployments \
            $(kubectl get deployments --sort-by=.metadata.creationTimestamp | grep ${{ env.DEPLOYMENT_NAME }} | awk '{ print $1 }' | head -n -2 | xargs) || true
